{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT_GPT_MODEL = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os.path import expanduser\n",
    "load_dotenv(os.path.join(expanduser(\"~\"), \".env\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"data_input.xlsx\"\n",
    "times = pd.read_excel(data_file, sheet_name=\"Data\", index_col=\"ID\")\n",
    "demog = pd.read_excel(data_file, sheet_name=\"Demographic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "times.columns = [\n",
    "    \"start_time\",\n",
    "    \"completion_time\",\n",
    "    \"star_rating\",\n",
    "    \"txt_what_liked\",\n",
    "    \"txt_what_not_liked\",\n",
    "    \"txt_do_to_improve\",\n",
    "    \"txt_anything_else\",\n",
    "    \"recommend_likelihood\",\n",
    "    \"ref_num\",\n",
    "    \"person_id\",\n",
    "    \"start_group\",\n",
    "    \"gender\",\n",
    "    \"age_group\",\n",
    "    \"country\",\n",
    "    \"province\",\n",
    "    \"number_finished\",\n",
    "    \"reg_day\",\n",
    "    \"reg_hour\",\n",
    "    \"PPA\",\n",
    "    \"has_result\",\n",
    "]\n",
    "\n",
    "demog.columns = [\n",
    "    \"person_id\",\n",
    "    \"start_group\",\n",
    "    \"gender\",\n",
    "    \"age_group\",\n",
    "    \"country\",\n",
    "    \"province\",\n",
    "    \"number_finished\",\n",
    "    \"reg_day\",\n",
    "    \"reg_hour\",\n",
    "    \"PPA\",\n",
    "    \"has_result\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.merge(\n",
    "    times, demog, on=\"person_id\", how=\"left\", suffixes=(None, \"_demog\")\n",
    ").convert_dtypes()\n",
    "\n",
    "all_data.drop(\n",
    "    columns=[\n",
    "        \"start_group\",\n",
    "        \"gender\",\n",
    "        \"age_group\",\n",
    "        \"country\",\n",
    "        \"province\",\n",
    "        \"number_finished\",\n",
    "        \"reg_day\",\n",
    "        \"reg_hour\",\n",
    "        \"PPA\",\n",
    "        \"has_result\",\n",
    "    ],\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data.astype(\n",
    "    {\n",
    "        \"start_group_demog\": \"category\",\n",
    "        \"gender_demog\": \"category\",\n",
    "        \"age_group_demog\": \"category\",\n",
    "        \"country_demog\": \"category\",\n",
    "        \"province_demog\": \"category\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleanup\n",
    "no_answer_text = \"no answer given\"\n",
    "\n",
    "txt_cols = [\n",
    "    \"txt_what_liked\",\n",
    "    \"txt_what_not_liked\",\n",
    "    \"txt_do_to_improve\",\n",
    "    \"txt_anything_else\",\n",
    "]\n",
    "\n",
    "all_data[txt_cols] = all_data[txt_cols].fillna(no_answer_text)\n",
    "\n",
    "for col in txt_cols:\n",
    "    all_data[col] = all_data[col].str.replace(r\"[^a-zA-Z ]\", \"\", regex=True) #not needed\n",
    "    all_data[col] = all_data[col].str.replace(\"\\n\", \" \") #remove line breaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data with no demographic records: 55 rows, 0.95% of 5784 total records.\n"
     ]
    }
   ],
   "source": [
    "len_times = len(times)\n",
    "lost_records = len(all_data) - len_times\n",
    "lost_records_percent = lost_records / len_times\n",
    "print(\n",
    "    f\"Data with no demographic records: {lost_records} rows, {lost_records_percent:.2%} of {len_times} total records.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT call for themes\n",
    "\n",
    "def summarize_gpt(responses):\n",
    "    # OpenAI API call to summarize the text\n",
    "\n",
    "    class Theme(BaseModel):\n",
    "        theme_text: str\n",
    "\n",
    "    class AllThemes(BaseModel):\n",
    "        themes: list[Theme]\n",
    "\n",
    "    messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"The following is a list of responses to a single question in a market research survey. \n",
    "                Create an overall list of themes extracted from all answers. There shoud be at most 12 themes, and they should have mninimal overlap. Each theme should be a maximum of 20 words.\n",
    "                Here are the responses{responses}\"\"\",\n",
    "            },\n",
    "        ]\n",
    "\n",
    "    completion   = client.beta.chat.completions.parse(\n",
    "        model=CHAT_GPT_MODEL,\n",
    "        #temperature = 0.5,\n",
    "        messages=messages,\n",
    "        response_format=AllThemes,\n",
    "    )\n",
    "\n",
    "    # Extract the summary from the response\n",
    "    return completion.choices[0].message.parsed\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT call for theme matching\n",
    "# https://platform.openai.com/docs/guides/structured-outputs\n",
    "def theme_matching(themes, responses):\n",
    "\n",
    "    class EachAnswer(BaseModel):\n",
    "        response_id: int\n",
    "        response_text: str\n",
    "        theme_id: int\n",
    "        theme_text: str\n",
    "\n",
    "    class AllAnswers(BaseModel):\n",
    "        classifications: list[EachAnswer]\n",
    "\n",
    "    messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an assistant for matching human responses to a survey to pre-existing themes.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"I have a list of themes summarised over some responses to a survey question. The themes represent common topics found in the resposnes.\n",
    "        Here are the themes: {themes}\"\"\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"I will give you the responses used to generate the themes.\n",
    "        For each response, I want you to identify which one of the themes most closely represents the response.\n",
    "        For each response, return the index of the response, the response text, the index and text of the most representative theme.\n",
    "        However, if the response text is {no_answer_text}, there will be no theme. Return 0 as the theme index and give the theme as 'no theme'.\n",
    "        It is critically important that there be as many classifications in the output as there are responses in the input.\n",
    "        Here are the responses {responses}:\"\"\",\n",
    "            },\n",
    "        ]\n",
    "    \n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=CHAT_GPT_MODEL,\n",
    "        temperature=0.1,\n",
    "        messages=messages,\n",
    "        response_format=AllAnswers,\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT call to summarise inputs\n",
    "\n",
    "def summarize_responses(inputs):\n",
    "    # OpenAI API call to summarize the text\n",
    "    response = client.chat.completions.create(\n",
    "        model=CHAT_GPT_MODEL,  # Specify the model you want to use\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant for summarising survey responses.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"You will be given a list of responses to a question in a survey. Your job is to extract key themes from the responses.\n",
    "                Ignore any responses that are very short, are empty, or have the text {no_answer_text}\n",
    "                Each theme should have a headline, followed by an explanatory paragraph. For each theme, provide from 1 to 3 verbatim quotes to illustrate the theme along side the explanatory paragraph.\n",
    "                Don't provide any duplicated verbatim quotes.\n",
    "                Sort the themes by their decreasing frequency of appearance. At the end, be sure to say which was the most commonly seen theme, and which was the least commonly seen.\n",
    "\n",
    "                Here are your inputs:\\n\\n{inputs}\"\"\",\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Extract the summary from the response\n",
    "    summary = response.choices[0].message.content.strip()\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text_column(dataframe, column_name):\n",
    "\n",
    "    print(f\"Column: {column_name}\")\n",
    "\n",
    "    response_list = dataframe[column_name].to_list()\n",
    "    expected_num_outputs = len(response_list)\n",
    "\n",
    "    # responses_input = \"\\n\".join(response_list)\n",
    "    responses_input_for_output = \"\".join([f\"{i}. {response} \\n\" for i, response in enumerate(response_list)])\n",
    "\n",
    "    responses_input = str(response_list)\n",
    "\n",
    "    with open(f\"outputs/A {column_name} responses_input.txt\", \"w\") as text_file:\n",
    "        text_file.write(responses_input_for_output)\n",
    "\n",
    "    output = summarize_gpt(responses_input)\n",
    "    # print(output.themes)\n",
    "    # print(len(output.themes))\n",
    "\n",
    "    themes_for_input = \"\\n\".join([theme.theme_text for theme in output.themes])\n",
    "    response = theme_matching(themes_for_input, response_list)\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    for i in range(1):\n",
    "        response = theme_matching(themes_for_input, response_list)\n",
    "        num_outputs = len(response.classifications)\n",
    "\n",
    "        print(\n",
    "            f\"Attempt {i}: expected, actual responses: {expected_num_outputs}, {num_outputs}\"\n",
    "        )\n",
    "\n",
    "        if num_outputs == expected_num_outputs:\n",
    "            break\n",
    "        else:\n",
    "            print(\n",
    "                f\"Mismatch, trying again\"\n",
    "            )\n",
    "\n",
    "            print(responses_input)\n",
    "            print([f'{el}\\n'for el in response.classifications])\n",
    "    else:\n",
    "        print(\"Retry iterations completed. No answers found.\")\n",
    "\n",
    "    \"\"\"\n",
    "    num_outputs = len(response.classifications)\n",
    "    print(\n",
    "        f\"Expected, actual responses: {expected_num_outputs}, {num_outputs}\"\n",
    "    )\n",
    "\n",
    "    with open(f\"outputs/B {column_name} classifications.txt\", \"w\") as text_file:\n",
    "        for el in response.classifications:\n",
    "            text_file.write(f\"{el.response_id} {el.response_text} {el.theme_id} {el.theme_text} \\n\")\n",
    "\n",
    "    classified_themes = [resp.theme_text for resp in response.classifications]\n",
    "\n",
    "    column_index = dataframe.columns.get_loc(column_name)\n",
    "    new_name = f'{column_name}_theme'\n",
    "    # dataframe.insert(column_index + 1, new_name, classified_themes)\n",
    "    print()\n",
    "\n",
    "    return response.classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_cols_to_classify = [\n",
    "    \"txt_what_liked\",\n",
    "    \"txt_what_not_liked\",\n",
    "    \"txt_do_to_improve\",\n",
    "]\n",
    "\n",
    "txt_cols_to_summarise = [\n",
    "    \"txt_do_to_improve\",\n",
    "    \"txt_anything_else\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: txt_what_not_liked\n",
      "Expected, actual responses: 43, 43\n",
      "\n"
     ]
    }
   ],
   "source": [
    "working_data = all_data[:43]\n",
    "response_list, classifications = None, None\n",
    "\n",
    "classifications = classify_text_column(\n",
    "    working_data,\n",
    "    \"txt_what_not_liked\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: txt_what_liked\n",
      "Expected, actual responses: 70, 70\n",
      "\n",
      "Column: txt_what_not_liked\n",
      "Expected, actual responses: 70, 70\n",
      "\n",
      "Column: txt_do_to_improve\n",
      "Expected, actual responses: 70, 69\n",
      "\n"
     ]
    }
   ],
   "source": [
    "working_data = all_data[:70]\n",
    "\n",
    "for col in txt_cols_to_classify:\n",
    "\n",
    "    classifications = classify_text_column(\n",
    "        working_data,\n",
    "        col,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transcriptions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
